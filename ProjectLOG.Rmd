---
title: "`biblio-bridge`: a context-based literature linker with emergent knowledge identification."
author: "T. J. Del Santo O'Neill"
date: "`r Sys.Date()`"
output: html_document
---

<style type="text/css">
  body{
  font-size: 16pt;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Concieving the idea

This application constructs a ***context-based*** similarity network from bibliographic data to evaluate citation bias<sup><a href="#footnote1">1</a></sup> and to identify potential knowledge gaps or opportunities for cross-disciplinary research. Specifically, `BiblioBridge` develops a network in which nodes represent scholarly papers and edges denote their semantic similarity. This context-based network is then compared with a ***citation-based*** network, where nodes correspond to papers and edges represent their citation relationships.

<!-- The computational workflow initialises with the focal text (*e.g.* paper) to assess and its associated bibliography (*e.g.* list of DOIs, titles...). `BiblioBridge` will then fetch text via APIs (*e.g.* Semantic Scholar, OpenAlex and/or PubMed) or PDFs and process them sequentially using NLP techniques (*e.g.* word embeddings or topic modelling). Users are able to set a "**depth level**", that is, the hierarchical layers or stages of citation analysis as `BiblioBridge` recursively explores the references cited by papers within a bibliography. Therefore, it measures how far the analysis extends into the citation network starting from an initial set of papers (e.g., a thesis bibliography); *e.g.* a $D_L=0$ means evaluating the focal text with its references. Compare network structures via *e.g.* Jaccard similarity and propose novel research avenues contingent upon depth level. -->

<p id="footnote1"><strong>1.</strong>That is, the tendency of authors to cite works based on their existing citation counts or perceived authority, rather than their actual contextual relevance to the citing content.</p>

## Computational workflow

`biblio-bridge` is an interoperable framework with different modules written in distinct programming languages (*e.g.* `R` and/or `Python`), tailored to the specific task at hand, so that the advantages of each language are fully exploited. The end result would be a web app via `shiny` and deploy it via the cloud-based platform `shinyapps.io`.

- Module 1: fetching API data (`Python`-based);
- Module 2: NLP metrics (`Python`-based via `NLTK` and `spaCy`); and
- Module 3: data analysis and vis (`R`-based).

```{r, echo=FALSE, warning=FALSE, message=FALSE,  fig.width=8, fig.height=6, out.width="100%"}
library(rjson)
ini.doi <- fromJSON(file = "resulting_metadata/initial_data_10.1111_faf.12817.json")
dl0.files <- list.files(path = "resulting_metadata/dl0/")
dl1.files <- list.files(path = "resulting_metadata/dl1/")
dl0.vec <- c()
dl0.year <- c()
dl1.vec <- c()
dl1.year <- c()
for (i in 1:length(dl0.files)) {
  file.data <- fromJSON(file = paste("resulting_metadata/dl0/", dl0.files[i], sep = ""))
  dl0.vec[i] <- length(file.data$referenced_works)
  dl0.year[i] <- file.data$publication_year
}
for (i in 1:length(dl1.files)) {
  file.data <- fromJSON(file = paste("resulting_metadata/dl1/", dl1.files[i], sep = ""))
  dl1.vec[i] <- length(file.data$referenced_works)
  dl1.year[i] <- file.data$publication_year
}
data <- data.frame(DL = as.factor(c("Focal", "DL0", "DL1", "DL2")), No = c(1, 89, 3857, 108546))
library(ggplot2)
extrafont::loadfonts(device = "all")
library(latex2exp)
# Personalised theme
theme_tjdso <- theme(
  text = element_text(family = "Consolas", size = 20),
  panel.background = element_blank(),
  panel.border = element_rect(fill = FALSE),
  panel.grid.major = element_line(
    linetype = 3,
    colour = "grey",
    size = 0.25
  ),
  panel.grid.minor = element_line(
    linetype = 3,
    colour = "grey",
    size = 0.1
  ),
  strip.background = element_blank(),
  strip.text = element_text(size = 20),
  strip.placement = "outside",
  panel.spacing.x = unit(5, "mm"),
  axis.ticks.length = unit(-2, "mm"),
  axis.text.x.top = element_blank(),
  axis.text.y.right = element_blank(),
  axis.title.x.top = element_blank(),
  axis.title.y.right = element_blank(),
  axis.title = element_text(size = 18),
  axis.text.x.bottom = element_text(margin = margin(4, 0, 0, 0, "mm")),
  axis.text.y.left = element_text(margin = margin(0, 4, 0, 0, "mm"))
)
mod <- lm(log10(data$No) ~ as.integer(data$DL)[c(2, 3, 4, 1)])
ggplot(data = data, mapping = aes(x = 1:4, y = No)) +
  geom_hline(
    yintercept = 100000,
    colour = "darkred",
    linetype = 2,
    linewidth = 1
  ) +
  geom_point(
    size = 8,
    fill = "steelblue",
    shape = 21,
    stroke = 1.25
  ) +
  geom_smooth(
    method = "lm",
    se = FALSE,
    linetype = 1,
    colour = "black"
  ) +
  scale_y_log10(
    breaks = scales::trans_breaks("log10", function(x)
      10^x),
    labels = scales::trans_format("log10", scales::math_format(10^.x))
  ) +
  scale_x_continuous(breaks = 1:4,
                     labels = c("Focal DOI", "DL = 0", "DL = 1", "DL = 2")) +
  labs(x = "Depth level", y = "Referenced works (No.)") +
  annotate(
    geom = "text",
    x = 1,
    y = 10^5.2,
    label = "Open Alex API rate limit",
    colour = "darkred",
    hjust = 0,
    family = "Consolas",
    size = 6
  ) +
  annotate(
    geom = "curve",
    x = 2.5,
    y = 10^2.5,
    xend = 2.75,
    yend = 10^1.5,
    arrow = arrow(length = unit(0.3, "cm"), type = "closed"),
    color = "grey75",
    size = 1.2,
    curvature = 0.3
  ) +
  annotate(
    geom = "text",
    x = 2.8,
    y = 10^1.5,
    label = TeX(r"(Increase of $\approx 1 \times 47^{DL}$)"),
    hjust = 0,
    family = "Consolas",
    size = 6
  ) +
  theme_tjdso 
```

### Citation-based net at DL=2 by domain

```{r echo=FALSE, warning=FALSE, message=FALSE,  fig.width=8, fig.height=6, out.width="100%"}
# Load required packages
library(jsonlite)
library(dplyr)
library(visNetwork)
library(stringr)

# Define paths
initial_json <- "resulting_metadata/initial_data_10.1111_faf.12817.json"
dl0_dir <- "resulting_metadata/dl0/"
dl1_dir <- "resulting_metadata/dl1/"

# Function to extract work ID from OpenAlex URL
extract_work_id <- function(id) {
  if (is.null(id) || is.na(id) || id == "") return(NA)
  str_extract(id, "W[0-9]+")
}

# Function to read a single JSON file and extract relevant fields
read_json_file <- function(file_path, depth) {
  tryCatch({
    json_data <- fromJSON(file_path, flatten = TRUE)
    ref_works <- if (is.null(json_data$referenced_works) || length(json_data$referenced_works) == 0) {
      character(0)
    } else {
      sapply(json_data$referenced_works, extract_work_id)
    }
    primary_topic <- json_data$primary_topic
    subfield_topic <- json_data$subfield_topic
    field_topic <- json_data$field_topic
    domain_topic <- json_data$domain_topic
    data.frame(
      id = extract_work_id(json_data$id),
      title = if (is.null(json_data$title)) "No title available" else json_data$title,
      publication_year = if (is.null(json_data$publication_year)) NA else json_data$publication_year,
      cited_by_count = if (is.null(json_data$cited_by_count)) 0 else json_data$cited_by_count,
      referenced_works = I(list(ref_works)),
      depth = depth,
      primary_topic = ifelse(is.null(primary_topic), "Unknown topic", primary_topic),
      subfield_topic = ifelse(is.null(primary_topic) || is.null(primary_topic), "Unknown subfield", subfield_topic),
      field_topic = ifelse(is.null(primary_topic) || is.null(primary_topic), "Unknown field", field_topic),
      domain_topic = ifelse(is.null(primary_topic) || is.null(primary_topic), "Unknown domain", domain_topic),
      stringsAsFactors = FALSE
    )
  }, error = function(e) {
    message("Error reading ", file_path, ": ", e$message)
    NULL
  })
}

# Read initial JSON (focal paper, depth 0)
initial_data <- read_json_file(file_path = initial_json, depth = 0)
if (is.null(initial_data)) stop("Failed to read initial JSON file.")

# Read all JSON files in dl0 (first-level references, depth 1)
dl0_files <- list.files(dl0_dir, pattern = "\\.json$", full.names = TRUE)
dl0_data <- lapply(dl0_files, function(f) read_json_file(f, depth = 1)) %>% bind_rows()

# Read all JSON files in dl1 (second-level references, depth 2)
dl1_files <- list.files(dl1_dir, pattern = "\\.json$", full.names = TRUE)
dl1_data <- lapply(dl1_files, function(f) read_json_file(f, depth = 2)) %>% bind_rows()

# Combine all data and remove duplicates by id
all_data <- bind_rows(initial_data, dl0_data, dl1_data) %>%
  filter(!is.na(id)) %>%
  distinct(id, .keep_all = TRUE)

# Create nodes data frame with domain-based grouping and sized by cited_by_count
nodes <- all_data %>%
  select(id, title, publication_year, cited_by_count, depth, primary_topic, subfield_topic, field_topic, domain_topic) %>%
  mutate(
    label = paste0(id, "\n", substr(title, 1, 20), "..."),
    # value = pmin(cited_by_count, 50),  # Size nodes by cited_by_count, capped at 50
    value = 1,
    title = paste0("<b>", title, "</b><br>Year: ", publication_year, "<br>Citations: ", cited_by_count),
    # Use domain_topic for grouping
    group = domain_topic
  )

# Create edges data frame with transparent colors
edges <- all_data %>%
  select(id, referenced_works) %>%
  tidyr::unnest(referenced_works) %>%
  filter(!is.na(referenced_works) & referenced_works != "") %>%
  rename(from = id, to = referenced_works) %>%
  filter(to %in% nodes$id) %>%
  mutate(
    color = case_when(
      from %in% nodes$id[nodes$depth == 0] ~ "rgba(255, 0, 0, 0.5)",  # Red for DL0 with 10% opacity
      from %in% nodes$id[nodes$depth == 1] ~ "rgba(0, 0, 255, 0.5)",  # Blue for DL1 with 10% opacity
      from %in% nodes$id[nodes$depth == 2] ~ "rgba(0, 255, 0, 0.5)",  # Green for DL2 with 10% opacity
      TRUE ~ "rgba(128, 128, 128, 0.5)"  # Default gray with 10% opacity
    ),
    width = 0.5,
    arrows = "to"
  )

# Create visNetwork visualization with force-directed layout for clustering
visNetwork(nodes, edges, width = "100%", height = "1200px") %>%
  visIgraphLayout() %>%  # Force-directed layout for clustering
  visNodes(
    shape = "dot",
    scaling = list(min = 10, max = 50),  # Scale nodes by cited_by_count
    font = list(size = 12)
  ) %>%
  visEdges(
    arrows = "to",
    smooth = list(enabled = TRUE, type = "curvedCW", roundness = 0.2),
    width = 0.5
  ) %>%
  visLegend(position = "right", main = "Domain Clusters") %>%
  # visLegend(addNodes = list(
  #   list(label = "Physical Sciences", shape = "ellipse", color = "#ADD8E6"),  # Light blue
  #   list(label = "Life Sciences", shape = "ellipse", color = "#FFFF99"),     # Light yellow
  #   list(label = "Health Sciences", shape = "ellipse", color = "#FFB6C1"),   # Light pink
  #   list(label = "Social Sciences", shape = "ellipse", color = "#90EE90"),   # Light green
  #   list(label = "Unknown domain", shape = "ellipse", color = "#DDA0DD")     # Plum
  # ), position = "right", main = "Domain Clusters") %>%
  visOptions(
    highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE),
    nodesIdSelection = TRUE
  ) %>%
  visInteraction(
    zoomView = TRUE,
    dragNodes = TRUE,
    hover = TRUE
  ) %>%
  visPhysics(
    stabilization = TRUE,
    barnesHut = list(
      gravitationalConstant = -8000,  # Stronger repulsion for clustering
      centralGravity = 0.1,          # Slight pull toward center
      springLength = 150,            # Adjust spacing
      springConstant = 0.05,         # Weaker springs for clustering
      damping = 0.9
    )
  )

```

